{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import email\n",
    "import email.policy\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data\n",
    "\n",
    "Download the compressed files from the Apache SpamAssassin’s public datasets, store them in the _Data_ folder and return the list of the decompressed e-mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files 2551.\n",
      "Number of files 250.\n",
      "Number of files 501.\n"
     ]
    }
   ],
   "source": [
    "def download_and_unzip_content(filename):\n",
    "    \n",
    "    files_arr = []     # array of mails\n",
    "    path = 'Data/'\n",
    "\n",
    "    # load and unzip the compressed files\n",
    "    try:               \n",
    "        tar = tarfile.open(path+filename)\n",
    "        print('Number of files {}.'.format(len(tar.getmembers()[1:])))\n",
    "\n",
    "        for file in tar.getmembers()[1:]:\n",
    "            f = tar.extractfile(file)\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "            text = email.parser.BytesParser(policy=email.policy.default).parsebytes(content)\n",
    "            files_arr.append(text)\n",
    "\n",
    "        tar.close()\n",
    "        return files_arr\n",
    " \n",
    "    # download the files is needed\n",
    "    except FileNotFoundError:    \n",
    "        print('File not found. Downloading it.')\n",
    "        url = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "        urlretrieve(url+filename, path+filename)\n",
    "        download_and_unzip_content(filename)\n",
    "        \n",
    "        \n",
    "\n",
    "easy_ham = download_and_unzip_content(\"20021010_easy_ham.tar.bz2\")\n",
    "hard_ham = download_and_unzip_content(\"20021010_hard_ham.tar.bz2\")\n",
    "spam = download_and_unzip_content(\"20021010_spam.tar.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preprocessing and Analysis\n",
    "\n",
    "### 2.1 Train test split\n",
    " Before performing any analysis we split the e-mails into a train and test set. Instead of using the function `train_test_split` from `sklearn.model_selection` we define a custom one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_set(ratio, spam, ham1, ham2, seed = 42):\n",
    "    \n",
    "    print(\"The output is the following: X_train, X_test, y_train, y_test.\")\n",
    "    print(\"1: spam, 0: ham.\")\n",
    "    \n",
    "    # Split easy/hard ham and spam into train and test datsets\n",
    "    X_train_spam = spam[int(ratio*len(spam)):]\n",
    "    X_train_ham1 = ham1[int(ratio*len(ham1)):]\n",
    "    X_train_ham2 = ham2[int(ratio*len(ham2)):]\n",
    "    \n",
    "    X_test_spam = spam[:int(ratio*len(spam))]\n",
    "    X_test_ham1 = ham1[:int(ratio*len(ham1))]\n",
    "    X_test_ham2 = ham2[:int(ratio*len(ham2))]\n",
    "    \n",
    "    # Join the train and test datsets\n",
    "    X_train = np.array(X_train_spam + X_train_ham1 + X_train_ham2)\n",
    "    X_test = np.array(X_test_spam + X_test_ham1 + X_test_ham2)\n",
    "    \n",
    "    # Shuffle the datasets\n",
    "    train_shuffle = np.random.RandomState(seed=seed).permutation(len(X_train))\n",
    "    test_shuffle = np.random.RandomState(seed=seed).permutation(len(X_test))\n",
    "    \n",
    "    # Generate the labels\n",
    "    y_train = np.concatenate([np.ones(len(X_train_spam)), np.zeros(len(X_train_ham1) + len(X_train_ham2))])\n",
    "    y_test = np.concatenate([np.ones(len(X_test_spam)), np.zeros(len(X_test_ham1) + len(X_test_ham2))])\n",
    "    \n",
    "    return X_train[train_shuffle], X_test[test_shuffle], y_train[train_shuffle], y_test[test_shuffle]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is the following: X_train, X_test, y_train, y_test.\n",
      "1: spam, 0: ham.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test , y_train, y_test = split_train_test_set(.2, spam, easy_ham, hard_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Words counting\n",
    "\n",
    "Let's parse the e-mails' content. \n",
    "\n",
    "First, we define a function that parses the html decorators from a the body of an e-mail and returns a string containing its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_mail(email):\n",
    "    mail_text = ''\n",
    "    \n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if ctype in ('text/html', 'text/plain'):   \n",
    "            try: \n",
    "                content = part.get_content()\n",
    "            except:\n",
    "                content = str(part.get_payload())   \n",
    "                \n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            mail_text += soup.get_text()\n",
    "            \n",
    "    return mail_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, collect the words. \n",
    "The following function makes the following parsing operations:\n",
    "\n",
    "- converts words to lower-case,\n",
    "- removes punctuation,\n",
    "- replaces numbers with `NUMBER`,\n",
    "- replaces urls with `URL`,\n",
    "\n",
    "It returns the list of all the words of the e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def collect_words(mail_string):     \n",
    "    \n",
    "    re_url1 = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}'\n",
    "    re_url2 = '|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}'\n",
    "    re_url3 = '|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    re_url = re_url1+re_url2+re_url3\n",
    "    \n",
    "    reg_1 = re.compile(re_url)             # -> Remove ULRs\n",
    "    reg_2 = re.compile(r'[0-9]+')          # -> Remove numbers\n",
    "    reg_3 = re.compile(r'[^\\w\\s]')         # -> Remove punctuation\n",
    "    reg_4 = re.compile(r'[\\n\\r\\t]')        # -> Remove newlines, tabs, etc..\n",
    "    \n",
    "    mail_string = mail_string.lower()      # -> Convert words in lower-case\n",
    "    mail_string = reg_1.sub(\"URL\", mail_string)\n",
    "    mail_string = reg_2.sub(\"NUMBER\", mail_string)\n",
    "    mail_string = reg_3.sub(\" \", mail_string)\n",
    "    mail_string = reg_4.sub(\" \", mail_string)\n",
    "    \n",
    "    return mail_string.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to __make a vocabulary with the word frequencies__.\n",
    "We neglect all the english common words like 'the' or 'a' or 'is'..., that are not useful to classify spam e-mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def make_dict(mail_set, rm_comm_eng_words, subj_dict=False): \n",
    "    dictionary = Counter()\n",
    "    \n",
    "    # if true makes a dictionary with the most common words of the subject\n",
    "    if subj_dict:\n",
    "        for mail in mail_set:\n",
    "            subj_string = str(mail['Subject'])\n",
    "            dictionary += Counter(collect_words(subj_string))\n",
    "    \n",
    "    # make a dictionary with the most common body words\n",
    "    else:\n",
    "        for mail in mail_set:\n",
    "            dictionary += Counter(collect_words(read_mail(mail))) \n",
    "    \n",
    "    # Remove the English most common words\n",
    "    single_letters = ['e','c','x','g','p','r','n','f','u','l','a','i']\n",
    "    for letter in single_letters:\n",
    "        del dictionary[letter]\n",
    "    \n",
    "    if rm_comm_eng_words:\n",
    "        common_words = np.array(stopwords.words('english'))\n",
    "        for word in common_words:\n",
    "            del dictionary[word]\n",
    "     \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate __3 different vocabnularies__: \n",
    "\n",
    "- ham words, \n",
    "- spam words,\n",
    "- suject words from spam.\n",
    "\n",
    "The following function generates the vocabulary via the `make_dict` function, it saves it in a `.txt` file.\n",
    "Finally, it loads the list of the `n_words` most common words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded.\n",
      "Dictionary loaded.\n",
      "Dictionary loaded.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')    # shut up bs4 URL warning\n",
    "\n",
    "\n",
    "def most_common_words(n_words, txt_file, words_set=X_train, rm_comm_eng_words=True, subj_dict=False,):\n",
    "    \n",
    "    # try to load the dictionary from the .txt file.\n",
    "    try:\n",
    "        body_dict = []\n",
    "        file = open(txt_file, 'r')\n",
    "        \n",
    "        for line in range(n_words):\n",
    "            body_dict.append(file.readline().rstrip())\n",
    "        \n",
    "        file.close()\n",
    "        body_dict = np.array(body_dict)\n",
    "        print('Dictionary loaded.')\n",
    "    \n",
    "    # make_dict if the .txt doesn't exists\n",
    "    except FileNotFoundError:\n",
    "        print(\"There's no dictionary. Creating a new one.\")\n",
    "        body_dict = make_dict(words_set, subj_dict=subj_dict, rm_comm_eng_words=rm_comm_eng_words)\n",
    "        body_dict = body_dict.most_common()\n",
    "        file = open(txt_file, 'w+')\n",
    "        \n",
    "        for key, freq in body_dict:\n",
    "            file.write(str(key)+'\\n')\n",
    "        \n",
    "        file.close()\n",
    "        body_dict = most_common_words(n_words, txt_file, rm_comm_eng_words)\n",
    "    \n",
    "    return body_dict\n",
    "\n",
    "\n",
    "\n",
    "spam_subj_words = most_common_words(50,'vocabularies/spam_subj_dict.txt', words_set=X_train[y_train==1], subj_dict=True)\n",
    "ham_words = most_common_words(200, 'vocabularies/ham_dict.txt', words_set=X_train[y_train==0])\n",
    "spam_words = most_common_words(250, 'vocabularies/spam_dict.txt', words_set=X_train[y_train==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 20 most common ham and spam words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Ham</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>URL</td>\n",
       "      <td>com</td>\n",
       "      <td>font</td>\n",
       "      <td>list</td>\n",
       "      <td>new</td>\n",
       "      <td>get</td>\n",
       "      <td>one</td>\n",
       "      <td>use</td>\n",
       "      <td>time</td>\n",
       "      <td>like</td>\n",
       "      <td>would</td>\n",
       "      <td>color</td>\n",
       "      <td>net</td>\n",
       "      <td>linux</td>\n",
       "      <td>mail</td>\n",
       "      <td>also</td>\n",
       "      <td>people</td>\n",
       "      <td>date</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Spam</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>free</td>\n",
       "      <td>URL</td>\n",
       "      <td>mv</td>\n",
       "      <td>money</td>\n",
       "      <td>email</td>\n",
       "      <td>mail</td>\n",
       "      <td>please</td>\n",
       "      <td>get</td>\n",
       "      <td>list</td>\n",
       "      <td>business</td>\n",
       "      <td>people</td>\n",
       "      <td>click</td>\n",
       "      <td>information</td>\n",
       "      <td>one</td>\n",
       "      <td>name</td>\n",
       "      <td>time</td>\n",
       "      <td>us</td>\n",
       "      <td>new</td>\n",
       "      <td>receive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1    2     3      4      5     6       7    8     9  \\\n",
       "Ham   NUMBER   URL  com  font   list    new   get     one  use  time   \n",
       "Spam  NUMBER  free  URL    mv  money  email  mail  please  get  list   \n",
       "\n",
       "            10      11     12           13     14    15    16      17    18  \\\n",
       "Ham       like   would  color          net  linux  mail  also  people  date   \n",
       "Spam  business  people  click  information    one  name  time      us   new   \n",
       "\n",
       "           19  \n",
       "Ham     world  \n",
       "Spam  receive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series,DataFrame\n",
    "\n",
    "ham_spam = DataFrame({'Ham': ham_words[:20], 'Spam': spam_words[:20]})\n",
    "ham_spam.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, words like __free__, __money__ or __please__ looks like typical spammy words. \n",
    "Now a look to the 10 most frequent spam subject words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Subj</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>ilug</td>\n",
       "      <td>free</td>\n",
       "      <td>adv</td>\n",
       "      <td>rates</td>\n",
       "      <td>get</td>\n",
       "      <td>home</td>\n",
       "      <td>money</td>\n",
       "      <td>best</td>\n",
       "      <td>systemworks</td>\n",
       "      <td>new</td>\n",
       "      <td>low</td>\n",
       "      <td>year</td>\n",
       "      <td>social</td>\n",
       "      <td>clearance</td>\n",
       "      <td>zzzz</td>\n",
       "      <td>one</td>\n",
       "      <td>insurance</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>online</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2    3      4    5     6      7     8            9  \\\n",
       "Subj  NUMBER  ilug  free  adv  rates  get  home  money  best  systemworks   \n",
       "\n",
       "       10   11    12      13         14    15   16         17        18  \\\n",
       "Subj  new  low  year  social  clearance  zzzz  one  insurance  mortgage   \n",
       "\n",
       "          19  \n",
       "Subj  online  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series,DataFrame\n",
    "\n",
    "subj_spam = DataFrame({'Subj': spam_subj_words[:20]})\n",
    "subj_spam.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Email type\n",
    "\n",
    "Now take a look at how the emails are constructed. Many mails consists in multipart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = email.get_content_type()\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the different e-mail types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multipart/alternative</th>\n",
       "      <th>multipart/mixed</th>\n",
       "      <th>multipart/related</th>\n",
       "      <th>multipart/report</th>\n",
       "      <th>multipart/signed</th>\n",
       "      <th>text/html</th>\n",
       "      <th>text/plain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Ham</td>\n",
       "      <td>40.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Spam</td>\n",
       "      <td>36.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      multipart/alternative  multipart/mixed  multipart/related  \\\n",
       "Ham                    40.0             12.0                3.0   \n",
       "Spam                   36.0             34.0                8.0   \n",
       "\n",
       "      multipart/report  multipart/signed  text/html  text/plain  \n",
       "Ham                1.0              55.0       94.0      2036.0  \n",
       "Spam               NaN               NaN      144.0       179.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_ser = Series(dict(structures_counter(X_train[y_train==0])))\n",
    "spam_ser = Series(dict(structures_counter(X_train[y_train==1])))\n",
    "\n",
    "struct_df = DataFrame({'Ham':ham_ser, 'Spam':spam_ser})\n",
    "struct_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a good feature to have. \n",
    "There's a lot of `plain text` in the ham dataset, while `html` is more frequent in the spam one.\n",
    "\n",
    "Let's save the different `email_types`s in the `category` list for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_structures = structures_counter(X_train)\n",
    "categories = np.array([key for (key, word) in email_structures.most_common()], dtype='<U13')\n",
    "categories = np.sort(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Generating the Features\n",
    "\n",
    "Now convert our features into a vector.\n",
    "We define two new classes, `DigitalizeMail` and `SelectFeatures`. \n",
    "\n",
    "The `transform()` method of  `DigitalizeMail`, create the feature vector.\n",
    "For each mail it sotres:\n",
    "\n",
    "- the frequency of the 200 most common ham words\n",
    "- the frequency of the 250 most common spam words\n",
    "- the frequency of the 50 most common subject words\n",
    "- the e-mail type\n",
    "\n",
    "However there are a lot of common words in the  ham and spam vocabulary. Therefore the feature vector has many repeated entries.\n",
    "To avoid repetitions and reduce the risk of overfitting, we use `SelectFeatures` that chooses only a subset of the features vector and takes care of avoiding repetitions. \n",
    "The total number of features is treated as a hyper-parameter. It is different for each trained classifier (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the feature vectors\n",
    "def vectorize(mail, dictionary, n_words, subject=False):\n",
    "    freq_vec = np.zeros(n_words)\n",
    "    mail_words = make_dict([mail], subj_dict=subject, rm_comm_eng_words=False)\n",
    "    \n",
    "    for index, key in list(enumerate(dictionary[:n_words])):\n",
    "        freq_vec[index] = mail_words[key]\n",
    "    \n",
    "    return freq_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# make the feature vector\n",
    "class DigitalizeMail(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, ham_dict, spam_dict, subj_dict): \n",
    "        self.ham_dict = ham_dict\n",
    "        self.spam_dict = spam_dict\n",
    "        self.subj_dict = subj_dict\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        ham_vec = np.zeros((len(X), 200))\n",
    "        spam_vec = np.zeros((len(X), 250))\n",
    "        subj_dict_vec = np.zeros((len(X), 50))\n",
    "        email_type = np.zeros((len(X), 1), dtype='<U13')\n",
    "        \n",
    "        for index, mail in enumerate(X):\n",
    "            ham_vec[index] = vectorize(mail, self.ham_dict, 200)\n",
    "            spam_vec[index] = vectorize(mail, self.spam_dict, 250)\n",
    "            subj_dict_vec[index] = vectorize(mail, self.subj_dict, 50, subject=True)\n",
    "            email_type[index] = mail.get_content_type()\n",
    "            \n",
    "        return np.concatenate([ham_vec, spam_vec, subj_dict_vec, email_type], axis=1)\n",
    "        \n",
    "\n",
    "# choose only a subset of the total features    \n",
    "class SelectFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, ham_dict, spam_dict, subj_dict, spam_ham_ratio, n_subj_words): \n",
    "        self.ham_dict = ham_dict\n",
    "        self.spam_dict = spam_dict\n",
    "        self.subj_dict = subj_dict\n",
    "        self.spam_ham_ratio = spam_ham_ratio\n",
    "        self.n_subj_words = n_subj_words\n",
    "        self.total_features = np.concatenate([ham_words,spam_words,spam_subj_words, categories])\n",
    "   \n",
    "    def fit(self, X, y=None):\n",
    "        count, index = 0, 0\n",
    "        spam = []\n",
    "        \n",
    "        # make a list of len=spam_ham_ratio containing most common spammy words not in 200 most common ham words\n",
    "        while(count<self.spam_ham_ratio)and(index<len(self.spam_dict)):\n",
    "            if self.spam_dict[index] not in self.ham_dict[:200]:\n",
    "                spam.append(self.spam_dict[index])\n",
    "                count +=1\n",
    "            index +=1\n",
    "\n",
    "        self.args = [np.argwhere(self.spam_dict==el) for el in spam]\n",
    "        self.args = np.array(self.args).flatten()+200\n",
    "        self.body_dict = np.concatenate([self.ham_dict[:200-self.spam_ham_ratio], spam])\n",
    "        \n",
    "        # List with the reduced set of features\n",
    "        self.red_feat_list = np.concatenate([list(range(200-self.spam_ham_ratio)), \n",
    "                                             self.args, \n",
    "                                             list(range(450, 450+self.n_subj_words)),\n",
    "                                             list(range(500,500+len(categories)))])\n",
    "        \n",
    "        #self.red_words_list = np.concatenate([ham_words_m, spam_words_m, subj_words_m], axis=1)\n",
    "        #self.red_feat_list = np.concatenate([self.red_words_list, categories], axis=1)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        ham_words_m = X[:, :200-self.spam_ham_ratio]\n",
    "        spam_words_m = X[:, self.args]\n",
    "        subj_words_m = X[:, 450:450+self.n_subj_words]\n",
    "        email_type_m = X[:, -1]\n",
    "        return np.concatenate([ham_words_m, spam_words_m, subj_words_m,\n",
    "                               email_type_m.reshape((len(X), 1))], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Automatising the process\n",
    "\n",
    "First preprocessing.\n",
    "\n",
    "Make the `X_train_prepared` matrix of the train dataset with all the possibile features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2642, 501)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prepared = DigitalizeMail(ham_dict=ham_words, \n",
    "                                  spam_dict=spam_words, \n",
    "                                  subj_dict=spam_subj_words).fit_transform(X_train)\n",
    "X_train_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second preprocessing.\n",
    "\n",
    "Create the `Pipeline` that __selects the features for each classifier__. We use  `OneHotEcoder` to deal with the _categorical_ `email_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('num', StandardScaler(), slice(0,-1)),\n",
    "    ('cat', OneHotEncoder(categories=[categories]), [-1])\n",
    "])\n",
    "\n",
    "\n",
    "feat_sel = Pipeline([\n",
    "    ('select_features', SelectFeatures(ham_dict=ham_words,spam_dict=spam_words, \n",
    "                                       subj_dict=spam_subj_words, spam_ham_ratio=35, n_subj_words=10)),\n",
    "    ('preprocess', preprocessing )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Train some model\n",
    "\n",
    "Let's train some classifier. \n",
    "We train 5 different modelf that work with different algorithms, to see the best one.\n",
    "We will merge some of them into a __voting classifier__.\n",
    "\n",
    "\n",
    "### 4.1 Dumb Model -- everything is ham\n",
    "\n",
    "Set a reference for the sccuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848221044663134"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ham = X_train[y_train == 0]\n",
    "len(X_train_ham) / len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the simplest classifier is quite high. \n",
    "\n",
    "Therefore we use the __f1 score__ to evaluate the performances of the trained classifers.\n",
    "Use `cross_val_score` splitting the train dataset in 5 subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "def print_scores(clf_cv, acc=True, prec=True, rec=True):\n",
    "    if acc:\n",
    "        print('accuracy: %.3f' % clf_cv['test_accuracy_score'].mean())\n",
    "    if prec:\n",
    "        print('precision: %.3f' % clf_cv['test_precision_score'].mean())\n",
    "    if rec:\n",
    "        print('recall: %.3f' % clf_cv['test_recall_score'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression\n",
    "\n",
    "Simple Linear Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.967\n",
      "precision: 0.902\n",
      "recall: 0.878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    ('pre', feat_sel),\n",
    "    ('reg', LogisticRegression(solver='liblinear', random_state=42))\n",
    "])\n",
    "\n",
    "lin_cv = cross_validate(log_reg, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(lin_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune some hyperparameter (including the features selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.899\n",
      "Best params: {'pre__select_features__n_subj_words': 7, 'pre__select_features__spam_ham_ratio': 9, 'reg__C': 12.250897038028766, 'reg__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, reciprocal\n",
    "\n",
    "param_distrib = {'reg__penalty': ['l2', 'l1'], \n",
    "                 \"reg__C\":uniform(.1, 20),\n",
    "                 \"pre__select_features__spam_ham_ratio\":list(range(5, 35)),\n",
    "                 \"pre__select_features__n_subj_words\": list(range(5, 50))}\n",
    "\n",
    "rnd_srch_log = RandomizedSearchCV(log_reg, param_distributions=param_distrib, cv=5, scoring='f1', \n",
    "                                  random_state=42, n_iter=100, n_jobs=-1, verbose=5,)\n",
    "\n",
    "rnd_srch_log.fit(X_train_prepared, y_train)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_log.best_score_)\n",
    "print('Best params:', rnd_srch_log.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performances of the fine tuned algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.969\n",
      "precision: 0.887\n",
      "recall: 0.913\n"
     ]
    }
   ],
   "source": [
    "log_reg = rnd_srch_log.best_estimator_\n",
    "\n",
    "lin_cv = cross_validate(log_reg, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(lin_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SGD Classifier\n",
    "\n",
    "Another kind of linear model|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.963\n",
      "precision: 0.927\n",
      "recall: 0.826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = Pipeline([\n",
    "    ('pre', feat_sel),\n",
    "    ('sgd', SGDClassifier(loss=\"log\", random_state=42))\n",
    "])\n",
    "\n",
    "sgd_cv = cross_validate(sgd_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(sgd_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune some hyperparameter (including the features selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.875\n",
      "Best params: {'pre__select_features__n_subj_words': 9, 'pre__select_features__spam_ham_ratio': 14, 'sgd__alpha': 0.002715581955282939, 'sgd__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, reciprocal\n",
    "\n",
    "param_distrib = {\"sgd__loss\": ['hinge', 'log','squared_hinge'], 'sgd__alpha': reciprocal(.0001, 1),\n",
    "                 \"pre__select_features__spam_ham_ratio\":list(range(5, 35)),\n",
    "                 \"pre__select_features__n_subj_words\": list(range(5, 50))}\n",
    "\n",
    "rnd_srch_sgd = RandomizedSearchCV(sgd_clf, param_distributions=param_distrib, \n",
    "                                  cv=5, scoring='f1', random_state=42,\n",
    "                                  n_iter=100, verbose=3, n_jobs=-1)\n",
    "\n",
    "rnd_srch_sgd.fit(X_train_prepared, y_train)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_sgd.best_score_)\n",
    "print('Best params:', rnd_srch_sgd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performances of the fine tuned algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.964\n",
      "precision: 0.936\n",
      "recall: 0.826\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = rnd_srch_sgd.best_estimator_\n",
    "\n",
    "sgd_cv = cross_validate(sgd_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(sgd_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.965\n",
      "precision: 0.932\n",
      "recall: 0.831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = Pipeline([\n",
    "    ('pre', feat_sel),\n",
    "    ('rnd', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "forest_cv = cross_validate(forest_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(forest_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good permonances! Now fine tune some hyperparameter (including the features selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.930\n",
      "Best params: {'pre__select_features__n_subj_words': 35, 'pre__select_features__spam_ham_ratio': 7, 'rnd__max_depth': 92.59423558429536, 'rnd__n_estimators': 116}\n"
     ]
    }
   ],
   "source": [
    "param_distrib = {\"rnd__n_estimators\": list(range(1,500)), \"rnd__max_depth\": reciprocal(2,100),\n",
    "                 \"pre__select_features__spam_ham_ratio\":list(range(5, 35)),\n",
    "                 \"pre__select_features__n_subj_words\": list(range(5, 50))}\n",
    "\n",
    "rnd_srch_forest = RandomizedSearchCV(forest_clf, param_distributions=param_distrib,\n",
    "                                     cv=5, scoring='f1', random_state=42,\n",
    "                                     n_iter=100, verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_forest.fit(X_train_prepared, y_train)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_forest.best_score_)\n",
    "print('Best params:', rnd_srch_forest.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performances of the fine tuned algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.980\n",
      "precision: 0.956\n",
      "recall: 0.908\n"
     ]
    }
   ],
   "source": [
    "forest_clf = rnd_srch_forest.best_estimator_\n",
    "\n",
    "forest_cv = cross_validate(forest_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(forest_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great performances! This is the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 KNCLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.937\n",
      "precision: 0.843\n",
      "recall: 0.721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn_clf = Pipeline([\n",
    "    ('pre', feat_sel),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "knn_cv = cross_validate(knn_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(knn_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor perfomances. Let's see if we can get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.848\n",
      "Best params: {'pre__select_features__spam_ham_ratio': 22, 'pre__select_features__n_subj_words': 19, 'knn__weights': 'distance', 'knn__n_neighbors': 4, 'knn__leaf_size': 47}\n"
     ]
    }
   ],
   "source": [
    "param_distrib = {\"knn__n_neighbors\": list(range(1, 10)), \n",
    "                 \"knn__weights\": ['uniform', 'distance'], \n",
    "                 \"knn__leaf_size\": list(range(20, 50)),\n",
    "                 \"pre__select_features__spam_ham_ratio\":list(range(5, 35)),\n",
    "                 \"pre__select_features__n_subj_words\": list(range(5, 50))}\n",
    "\n",
    "rnd_srch_knn = RandomizedSearchCV(knn_clf, param_distributions=param_distrib,\n",
    "                                     cv=5, scoring='f1', random_state=42,\n",
    "                                     n_iter=100, verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_knn.fit(X_train_prepared, y_train)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_knn.best_score_)\n",
    "print('Best params:', rnd_srch_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performances of the fine tuned algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.955\n",
      "precision: 0.872\n",
      "recall: 0.828\n"
     ]
    }
   ],
   "source": [
    "knn_clf = rnd_srch_knn.best_estimator_\n",
    "\n",
    "knn_cv = cross_validate(knn_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(knn_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still very poor precision.\n",
    "\n",
    "### 4.6 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.961\n",
      "precision: 0.946\n",
      "recall: 0.788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = Pipeline([\n",
    "    ('pre', feat_sel),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "svc_cv = cross_validate(svc_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(svc_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune some hyperparameter (including the features selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.895\n",
      "Best params: {'pre__select_features__n_subj_words': 9, 'pre__select_features__spam_ham_ratio': 16, 'svc__C': 8.586243332726877, 'svc__gamma': 0.002593837224366831, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "param_distrib = {'svc__kernel': ['rbf', 'poly'],\n",
    "                 'svc__C': uniform(1,20),\n",
    "                 'svc__gamma': reciprocal(.0001, .1),\n",
    "                 \"pre__select_features__spam_ham_ratio\":list(range(5, 35)),\n",
    "                 \"pre__select_features__n_subj_words\": list(range(5, 50))}\n",
    "                 \n",
    "\n",
    "rnd_srch_svc = RandomizedSearchCV(svc_clf, param_distributions=param_distrib,\n",
    "                                     cv=5, scoring='f1', n_iter=100,\n",
    "                                     verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_svc.fit(X_train_prepared, y_train)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_svc.best_score_)\n",
    "print('Best params:', rnd_srch_svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.970\n",
      "precision: 0.948\n",
      "recall: 0.851\n"
     ]
    }
   ],
   "source": [
    "svc_clf = rnd_srch_svc.best_estimator_\n",
    "\n",
    "svc_cv = cross_validate(svc_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(svc_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good performances\n",
    "\n",
    "### 4.7 Voting Classifier\n",
    "\n",
    "Make a voting classifier that merges the predictions of the trained models.\n",
    "Given that `RandomForestClassifier` is by far the best one, we count its vote twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.979\n",
      "precision: 0.963\n",
      "recall: 0.895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "voting_clf = VotingClassifier([('log', log_reg), ('SGD', sgd_clf), ('forest_1', forest_clf),\n",
    "                               ('forest_2', forest_clf), ('knn', knn_clf), ('svc', svc_clf)], voting='hard')\n",
    "\n",
    "voting_cv = cross_validate(voting_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(voting_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Let's this with the individual classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_reg</th>\n",
       "      <th>sgd_clf</th>\n",
       "      <th>forest_clf</th>\n",
       "      <th>knn_clf</th>\n",
       "      <th>svc_clf</th>\n",
       "      <th>voting_clf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Precision</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Recall</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           log_reg  sgd_clf  forest_clf  knn_clf  svc_clf  voting_clf\n",
       "Accuracy     0.969    0.964       0.980    0.955    0.970       0.976\n",
       "Precision    0.887    0.936       0.956    0.872    0.948       0.969\n",
       "Recall       0.913    0.826       0.908    0.828    0.851       0.868"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs_names = ['log_reg', 'sgd_clf', 'forest_clf', 'knn_clf', 'svc_clf', 'voting_clf']\n",
    "cvs_list = [lin_cv, sgd_cv, forest_cv, knn_cv, svc_cv, voting_cv]\n",
    "\n",
    "cv_scores_dict = {name: (round(cv['test_accuracy_score'].mean(), 3), \n",
    "                      round(cv['test_precision_score'].mean(), 3), \n",
    "                      round(cv['test_recall_score'].mean(), 3))\n",
    "               for name, cv in zip(clfs_names, cvs_list)}\n",
    "    \n",
    "cv_scores_df = DataFrame(cv_scores_dict, index=['Accuracy', 'Precision', 'Recall'])\n",
    "cv_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomForestClassifier` performs better in accuracy but worse in precision.  \n",
    "\n",
    "Also __Log-reg__ and __KNN__ that perform very poorly in precision. __Remove them from the voting classifier__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.979\n",
      "precision: 0.968\n",
      "recall: 0.893\n"
     ]
    }
   ],
   "source": [
    "del voting_clf.estimators[-2], voting_clf.estimators[1]\n",
    "\n",
    "voting_cv = cross_validate(voting_clf, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(voting_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performances on the Test \n",
    "\n",
    "Preprocess the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared = DigitalizeMail(ham_dict=ham_words, \n",
    "                                  spam_dict=spam_words, \n",
    "                                  subj_dict=spam_subj_words).transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the voting classifier in the whole train set and evaluate the perforamances on the test set. \n",
    "For the final evaluation we use also the __Recall__ and __Precision scores__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the test set: 0.977\n",
      "Precision: 0.978\n",
      "Recall: 0.870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "voting_clf.fit(X_train_prepared, y_train)\n",
    "y_test_pred = voting_clf.predict(X_test_prepared)\n",
    "print('Score on the test set: %.3f' % accuracy_score(y_test, y_test_pred))\n",
    "print('Precision: %.3f' % precision_score(y_test, y_test_pred))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good! __Precision__ is quite high.\n",
    "Now take a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[558   2]\n",
      " [ 13  87]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow __only 2 false positive__ (ham classified as spam).\n",
    "\n",
    "Take a closer look at FP mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** MAIL NUMBER 103 ****\n",
      "\n",
      "Ryanair in partnership with Primary Insurance\n",
      "offer excellent value travel insurance from\n",
      "£7.00GBP/9.00 Euro per person for 31 day cover.\n",
      "\n",
      "Annual travel insurance* from £45.00GBP/63.00 Euro,\n",
      "includes 24 days winter sports cover !\n",
      "\n",
      "Our travel insurance provides a high standard of cover.\n",
      "\n",
      "Summary of Cover\n",
      "\t\n",
      "Medical Expenses up to £2 million\n",
      "Personal Liability  up to £2 million\n",
      "Personal Effects & Baggage up to £750\n",
      "Personal Accident Maximum Benefit £15,000\n",
      "Hospital Benefit up to £300\n",
      "Cancellation up to £500\n",
      "Curtailment up to £500\n",
      "Travel Delay up to £60\n",
      "Missed Departure up to £300\n",
      "Personal Money up to £500\n",
      "Legal Expenses up to £5000\n",
      "Holiday Abandonment up to £500\n",
      "\n",
      "All figures in Sterling pounds\n",
      "\n",
      "To book your Primary travel insurance policy\n",
      "click http://www.primarytrade.co.uk/internetsales/ryanair/\n",
      "\n",
      "or call Ryanair Direct Reservations now on:\n",
      "\n",
      "0871 246 0002 (UK)\t0818 304 304 (IRELAND)\n",
      "\n",
      "Primary Insurance also offer excellent rates on\n",
      "\n",
      "Golf Insurance from £15.00GBP/19.00 Euro\n",
      "per passenger for 31 day cover\n",
      "\n",
      "Ski insurance from £35.00GBP/45.00 Euro\n",
      "per passenger for 31 day cover\n",
      "\n",
      "Cover is only available to habitual residents of the UK and Ireland.\n",
      "\n",
      "*only available through the web.\n",
      "\n",
      "\n",
      "====================================================================\n",
      "\n",
      "E-MAIL DISCLAIMER\n",
      "\n",
      "This e-mail and any files and attachments transmitted with it\n",
      "are confidential and may be legally privileged. They are intended\n",
      "solely for the use of the intended recipient.  Any views and\n",
      "opinions expressed are those of the individual author/sender\n",
      "and are not necessarily shared or endorsed by Ryanair Holdings plc\n",
      "or any associated or related company. In particular e-mail\n",
      "transmissions are not binding for the purposes of forming\n",
      "a contract to sell airline seats, directly or via promotions,\n",
      "and do not form a contractual obligation of any type.\n",
      "Such contracts can only be formed in writing by post or fax,\n",
      "duly signed by a senior company executive, subject to approval\n",
      "by the Board of Directors.\n",
      "\n",
      "The content of this e-mail or any file or attachment transmitted\n",
      "with it may have been changed or altered without the consent\n",
      "of the author.  If you are not the intended recipient of this e-mail,\n",
      "you are hereby notified that any review, dissemination, disclosure,\n",
      "alteration, printing, circulation or transmission of, or any\n",
      "action taken or omitted in reliance on this e-mail or any file\n",
      "or attachment transmitted with it is prohibited and may be unlawful.\n",
      "\n",
      "If you have received this e-mail in error\n",
      "please notify Ryanair Holdings plc by emailing postmaster@ryanair.ie\n",
      "or contact Ryanair Holdings plc, Dublin Airport, Co Dublin, Ireland.\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "E-MAIL DISCLAIMER\n",
      "\n",
      "This e-mail and any files and attachments transmitted with it \n",
      "are confidential and may be legally privileged. They are intended \n",
      "solely for the use of the intended recipient.  Any views and \n",
      "opinions expressed are those of the individual author/sender \n",
      "and are not necessarily shared or endorsed by Ryanair Holdings plc \n",
      "or any associated or related company. In particular e-mail \n",
      "transmissions are not binding for the purposes of forming \n",
      "a contract to sell airline seats, directly or via promotions, \n",
      "and do not form a contractual obligation of any type.   \n",
      "Such contracts can only be formed in writing by post or fax, \n",
      "duly signed by a senior company executive, subject to approval \n",
      "by the Board of Directors.\n",
      "\n",
      "The content of this e-mail or any file or attachment transmitted \n",
      "with it may have been changed or altered without the consent \n",
      "of the author.  If you are not the intended recipient of this e-mail, \n",
      "you are hereby notified that any review, dissemination, disclosure, \n",
      "alteration, printing, circulation or transmission of, or any \n",
      "action taken or omitted in reliance on this e-mail or any file \n",
      "or attachment transmitted with it is prohibited and may be unlawful.\n",
      "\n",
      "If you have received this e-mail in error \n",
      "please notify Ryanair Holdings plc by emailing postmaster@ryanair.ie\n",
      "or contact Ryanair Holdings plc, Dublin Airport, Co Dublin, Ireland.  \n",
      "\n",
      "\n",
      "---\n",
      "You are currently subscribed to customers as: zzz-ryanair@example.com\n",
      "To unsubscribe send a blank email to leave-customers-949326K@mail.ryanairmail.com\n",
      "\n",
      "\n",
      "\n",
      "**** MAIL NUMBER 256 ****\n",
      "\n",
      "OpenText社\n",
      "伊東様\n",
      "\n",
      "いつもお世話になっております。\n",
      "安井@infocomです。\n",
      "\n",
      "あるタスクリストに、適当なマイルストーンを複数、タスクを複数用意し\n",
      "それぞれのタスクについては、存在する適当なマイルストーンに割り当てたものと\n",
      "します。\n",
      "あるマイルストーンを見ると、添付の絵にあるような画面が表示されるのですが\n",
      "ここで丸で囲った部分（ 期間 ）は、何を意味しているのでしょうか？\n",
      "\n",
      "つまり、３７日 、２７日\n",
      "のそれぞれの意味（算出のされ方について教えてください。）\n",
      "\n",
      "また「週末を除く」とありますが、週末を除かない設定方法はあるのでしょうか？\n",
      "\n",
      "以上、ご回答の方よろしくお願いいたします。\n",
      "\n",
      "\n",
      "--\n",
      "+--------------------------------------+\n",
      "インフォコム(株)\n",
      "ナレッジマネジメント本部\n",
      "KMフロンティア部\n",
      "コラボレーティブシステムグループ\n",
      "\n",
      "〒101-0062\n",
      "東京都千代田区神田駿河台3-11三井住友海上駿河台別館5F\n",
      "安井  剛\n",
      "E-mail: go@infocom.co.jp\n",
      "TEL: 03-3518-3299\n",
      "FAX: 03-3518-3055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "false_negative = (y_test==0)&(y_test_pred==1)\n",
    "fp_neg = np.argwhere(false_negative).flatten()\n",
    "\n",
    "for el in fp_neg:\n",
    "    print('**** MAIL NUMBER {} ****\\n'.format(el))\n",
    "    print(read_mail(X_test[el]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are __1 advertising e-mail__ and a mail written in __Japanese__ (unreadable).\n",
    "\n",
    "### 5.1 Performances of the individual classifiers\n",
    "\n",
    "Check the performances of the individual classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_reg</th>\n",
       "      <th>sgd_clf</th>\n",
       "      <th>forest_clf_acc</th>\n",
       "      <th>knn_clf</th>\n",
       "      <th>svc_clf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Precision</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Recall</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           log_reg  sgd_clf  forest_clf_acc  knn_clf  svc_clf\n",
       "Accuracy     0.961    0.959           0.979    0.942    0.965\n",
       "Precision    0.870    0.901           0.967    0.804    0.933\n",
       "Recall       0.870    0.820           0.890    0.820    0.830"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores_dict = {}\n",
    "\n",
    "for clf, name in zip([log_reg, sgd_clf, forest_clf, knn_clf, svc_clf], \n",
    "                     ['log_reg', 'sgd_clf', 'forest_clf_acc', 'knn_clf', 'svc_clf']):\n",
    "    clf.fit(X_train_prepared, y_train)\n",
    "    y_test_pred = clf.predict(X_test_prepared)\n",
    "    test_scores_dict[name] = (round(accuracy_score(y_test, y_test_pred), 3), \n",
    "                              round(precision_score(y_test, y_test_pred), 3),\n",
    "                              round(recall_score(y_test, y_test_pred), 3)) \n",
    "    \n",
    "test_scores_df = DataFrame(test_scores_dict, index=['Accuracy', 'Precision', 'Recall'])\n",
    "test_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Voting Classifier outperforms all the individual ones__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Features importance\n",
    "\n",
    "Take a look at the most important features.\n",
    "\n",
    "Use the `Features importance` method of the `RandomForestClassifier` algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = 'images/'+fig_id+'.'+fig_extension\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the features that contributed more than $1\\%$ to the decision process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = forest_clf['rnd'].feature_importances_\n",
    "above_1pct_scores = scores[scores>.01]*100\n",
    "\n",
    "total_feat = forest_clf['pre']['select_features'].total_features    # -> List of the total features used\n",
    "red_feat_pos = forest_clf['pre']['select_features'].red_feat_list   # -> Position of the reduced set of features\n",
    "red_feat = total_feat[red_feat_pos]                                 # -> List of the reduced set of features\n",
    "above_1pct_red_feat = red_feat[np.argwhere(scores>.01)].flatten()   # -> List of the most important features\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.axes()\n",
    "(markers, stemlines, baseline) = ax.stem(list(range(len(above_1pct_scores))),\n",
    "                                         above_1pct_scores, use_line_collection=True)\n",
    "\n",
    "plt.setp(markers, marker='o', markersize=15, markeredgecolor=\"olive\", markeredgewidth=1)\n",
    "plt.setp(stemlines, linestyle=\"-\", color=\"olive\", linewidth=1)\n",
    "\n",
    "\n",
    "# x - axis\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.set_xlim((-.5, len(above_1pct_scores)))\n",
    "\n",
    "# y - axis\n",
    "y_max = max(above_1pct_scores)+2\n",
    "ax.axes.set_ylim((0, y_max))\n",
    "old_ticks = ax.axes.get_yticks()\n",
    "ticks_labels = ['{}%'.format(digit) for digit in old_ticks]\n",
    "ax.axes.set_yticklabels(ticks_labels)\n",
    "ax.axes.set_ylabel('Importance')\n",
    "ax.axes.set_title('Features importance')\n",
    "\n",
    "ax.annotate('', xy=(10.8, -.5), xytext=(13.2,-.5),\n",
    "            arrowprops=dict(arrowstyle='<->', facecolor='red'),\n",
    "            annotation_clip=False)\n",
    "\n",
    "ax.annotate('email type', xy=(11.6, -1), annotation_clip=False)\n",
    "\n",
    "for index, feature in enumerate(above_1pct_red_feat):\n",
    "    ax.annotate(feature, xy=(index, above_1pct_scores[index]+.5), annotation_clip=False, rotation=50)\n",
    "\n",
    "save_fig('feature_importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Data Clustering\n",
    "\n",
    "Let's see how the e-mails dispose in the space of the most important features. \n",
    "Use the __t-SNE__ to reduce the dimensionality of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.base import clone\n",
    "\n",
    "pca_tsne = Pipeline([\n",
    "    (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
    "    (\"tsne\", TSNE(n_components=2, random_state=42)),\n",
    "])\n",
    "\n",
    "forest_prep = clone(forest_clf['pre'])\n",
    "X_prep_forest = forest_prep.fit_transform(X_train_prepared)\n",
    "\n",
    "X_pca_tsne_reduced = pca_tsne.fit_transform(X_prep_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "sne_ham = ax.scatter(X_pca_tsne_reduced[y_train==0][:,0], X_pca_tsne_reduced[y_train==0][:, 1], s=10, alpha=.8)\n",
    "sne_ham.set_label('Spam')\n",
    "\n",
    "sne_hspam = ax.scatter(X_pca_tsne_reduced[y_train==1][:,0], \n",
    "                       X_pca_tsne_reduced[y_train==1][:, 1], \n",
    "                       c='olive', s=10, alpha=.5)\n",
    "sne_hspam.set_label('Ham')\n",
    "\n",
    "ax.legend(fontsize='x-large')\n",
    "\n",
    "save_fig('clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
